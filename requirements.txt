# HuggingFace Transformers for LLM inference
transformers
accelerate

# Flash Attention 2 for faster inference (install with: pip install flash-attn --no-build-isolation)
# flash-attn

# FOR GPU USAGE: pip install torch --index-url https://download.pytorch.org/whl/cu121

# LlamaIndex core for RAG orchestration
llama-index-core

# FAISS vector store integration for LlamaIndex
llama-index-vector-stores-faiss

# HuggingFace embedding integration for LlamaIndex
llama-index-embeddings-huggingface

# Sentence Transformers for local embedding models
sentence-transformers

# FAISS library for efficient similarity search (CPU version)
faiss-cpu

# BM25 retriever for LlamaIndex
llama-index-retrievers-bm25

# ChromaDB vector store integration for LlamaIndex

# llama-index-vector-stores-chroma chromadb
# not working? ^